---
title: 问题记录
category: Operations Guide
---

###  1. [Ranger] : 启动ranger admin报错：[E] 007-updateBlankPolicyName.sql import failed!
更改msyql数据的参数：

临时方案：

```sql
MySQL [(none)]> show variables like 'log_bin_trust_function_creators';
+---------------------------------+-------+
| Variable_name                   | Value |
+---------------------------------+-------+
| log_bin_trust_function_creators | OFF   |
+---------------------------------+-------+
1 row in set (0.00 sec)

MySQL [(none)]> set global log_bin_trust_function_creators=1;
Query OK, 0 rows affected (0.00 sec)

MySQL [(none)]> show variables like 'log_bin_trust_function_creators';
+---------------------------------+-------+
| Variable_name                   | Value |
+---------------------------------+-------+
| log_bin_trust_function_creators | ON    |
+---------------------------------+-------+
1 row in set (0.00 sec)

```

但此方案在mysql重启后会更改回来，因此记得在my.cnf配置文件中添加：
log_bin_trust_function_creators=1

---
### 2. [Ambari] : 启动ambari agent后，Service和host状态提示黄色问号

mbari 所有Service提示黄色问号，一般为heartbeat lost引起，而这种情况为python版本问题

解决方法：

vim /etc/ambari-agent/conf/ambari-agent.ini
```
[security]
force_https_protocol=PROTOCOL_TLSv1_2
```

下面这个参数一般情况不用改
vim /etc/python/cert-verification.cfg
```
[https]
#verify=platform_default
verify=disable
```
verify这里有可能是enable或platform_default.将它改为disable，然后重启ambari agent，就可以正常连接了。

---
### 3. [Environment] : python 问题，unknown locale: UTF-8解决方法

https://www.phodal.com/blog/unknown-locale-utf-8/

报错如下:
```
Traceback (most recent call last):
  File "/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-INSTALL/scripts/hook.py", line 37, in <module>
    BeforeInstallHook().execute()
  File "/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py", line 375, in execute
    method(env)
  File "/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-INSTALL/scripts/hook.py", line 34, in hook
    install_packages()
  File "/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-INSTALL/scripts/shared_initialization.py", line 37, in install_packages
    retry_count=params.agent_stack_retry_count)
  File "/usr/lib/ambari-agent/lib/resource_management/core/base.py", line 125, in __new__
    cls(names_list.pop(0), env, provider, **kwargs)
  File "/usr/lib/ambari-agent/lib/resource_management/core/base.py", line 166, in __init__
    self.env.run()
  File "/usr/lib/ambari-agent/lib/resource_management/core/environment.py", line 160, in run
    self.run_action(resource, action)
  File "/usr/lib/ambari-agent/lib/resource_management/core/environment.py", line 124, in run_action
    provider_action()
  File "/usr/lib/ambari-agent/lib/resource_management/core/providers/package/__init__.py", line 53, in action_install
    self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)
  File "/usr/lib/ambari-agent/lib/resource_management/core/providers/package/yumrpm.py", line 256, in install_package
    if is_upgrade or use_repos or not self._check_existence(name):
  File "/usr/lib/ambari-agent/lib/resource_management/core/providers/package/yumrpm.py", line 309, in _check_existence
    return self.yum_check_package_available(name)
  File "/usr/lib/ambari-agent/lib/resource_management/core/providers/package/yumrpm.py", line 324, in yum_check_package_available
    package_list = yb.rpmdb.simplePkgList()
  File "/usr/lib/python2.7/site-packages/yum/__init__.py", line 1087, in <lambda>
    rpmdb = property(fget=lambda self: self._getRpmDB(),
  File "/usr/lib/python2.7/site-packages/yum/__init__.py", line 654, in _getRpmDB
    self._rpmdb = rpmsack.RPMDBPackageSack(root=self.conf.installroot,
  File "/usr/lib/python2.7/site-packages/yum/__init__.py", line 1083, in <lambda>
    conf = property(fget=lambda self: self._getConfig(),
  File "/usr/lib/python2.7/site-packages/yum/__init__.py", line 382, in _getConfig
    startupconf.pluginconfpath,disabled_plugins,enabled_plugins)
  File "/usr/lib/python2.7/site-packages/yum/__init__.py", line 636, in doPluginSetup
    plugin_types, confpath, disabled_plugins, enabled_plugins)
  File "/usr/lib/python2.7/site-packages/yum/plugins.py", line 166, in __init__
    self.run('config')
  File "/usr/lib/python2.7/site-packages/yum/plugins.py", line 188, in run
    func(conduitcls(self, self.base, conf, **kwargs))
  File "/usr/lib/yum-plugins/langpacks.py", line 202, in config_hook
    (lang, encoding) = locale.getdefaultlocale()
  File "/usr/lib64/python2.7/locale.py", line 511, in getdefaultlocale
    return _parse_localename(localename)
  File "/usr/lib64/python2.7/locale.py", line 443, in _parse_localename
    raise ValueError, 'unknown locale: %s' % localename
ValueError: unknown locale: UTF-8
```
使用下面语句测试是否下正常：
python -c 'import locale; print(locale.getdefaultlocale());'

解决方案：
在.bashrc中假如如下代码：
```
export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8
```
重启ambari-agent。

---
### 4. [Zookeeper] :  zookeeper启动报错：Could not configure server because SASL configuration did not allow the ZooKeeper server to authenticate itself properly: javax.security.auth.login.LoginException: Receive timed out

或者使用hadoop命令：
No valid credentials provided (Mechanism level: Receive timed out)
解决方案：
https://community.hortonworks.com/content/supportkb/149955/errorcould-not-configure-server-because-sasl-confi.html

```
 krb5.conf文件中添加：
[libdefaults]
udp_preference_limit = 1
```

---
### 5. [Environment] :  添加jce，否者kerberos有问题

下载jce并解压至JAVA_HOME/jre/lib/security目录下，AMbari所有节点均需要
http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html
unzip -o -j -q jce_policy-8.zip -d   $JAVA_HOME/jre/lib/security

---
### 6. [Ambari] : hive view查询报错 “S020 Data storage error ”

报错日志：
```
S020 Data storage error
[blahblah]
 
Caused by: javax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'ambari.DS_JOBIMPL_7' doesn't exist
Error Code: 1146
```

原因:
在mysql中创建元数据表时报错了
原语句如下 
```
CREATE TABLE DS_JOBIMPL_7 (
    ds_id character varying(255) NOT NULL,
    ds_applicationid character varying(3000),
    ds_conffile character varying(3000),
    ds_dagid character varying(3000),
    ds_dagname character varying(3000),
    ds_database character varying(3000),
    ds_datesubmitted bigint,
    ds_duration bigint,
    ds_forcedcontent character varying(3000),
    ds_globalsettings character varying(3000),
    ds_logfile character varying(3000),
    ds_owner character varying(3000),
    ds_queryfile character varying(3000),
    ds_queryid character varying(3000),
    ds_referrer character varying(3000),
    ds_sessiontag character varying(3000),
    ds_sqlstate character varying(3000),
    ds_status character varying(3000),
    ds_statusdir character varying(3000),
    ds_statusmessage character varying(3000),
    ds_title character varying(3000),
    PRIMARY KEY (ds_id)
);
```

报错：
```
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs
```

临时方案：
在hive中创建表：
```
CREATE TABLE DS_JOBIMPL_14 ( 
DS_id VARCHAR(255) NOT NULL, 
S_applicationId TEXT, 
DS_confFile TEXT, 
DS_dagId TEXT, 
DS_dagName TEXT, 
DS_dataBase TEXT, 
DS_dateSubmitted BIGINT, 
DS_duration BIGINT,
DS_forcedContent TEXT,
DS_globalSettings TEXT,
DS_guid TEXT, 
DS_hiveQueryId TEXT, 
DS_logFile TEXT, 
DS_owner TEXT, 
DS_queryFile TEXT, 
DS_queryId TEXT,
DS_referrer TEXT, 
DS_sessionTag TEXT,
DS_sqlState TEXT, 
DS_status TEXT, 
DS_statusDir TEXT, 
DS_statusMessage TEXT, 
DS_title TEXT,
DS_applicationId TEXT, 
PRIMARY KEY (ds_id));
```

每个用户需要一个对应的表，有多少用户尝试登陆过，可以在下面表中查询：
```
select * from ambari_sequences;
···
+-------------------------------------+----------------+
| sequence_name                       | sequence_value |
+-------------------------------------+----------------+
| ds_jobimpl_14_id_seq                |             50 |
| ds_jobimpl_7_id_seq                 |             50 |
+-------------------------------------+----------------+
···

```

---
### 7. [Ambari] : file view等报错

类似：
Service 'ats' check failed: Server Error
Service 'hdfs' check failed: Can't get Kerberos realm
Service 'userhome' check failed: HdfsApi connection failed. Check "webhdfs.url" property

原因：在amberi-server节点没有配置kerberos客户端，或krb5.conf没有配置正确


hadoop fs -mkdir /user/admin
hadoop fs -chown admin:hdfs /user/admin
hadoop fs -chmod 755/user/admin


### 8. [Kerberos] : 清空SSSD缓存

```
sss_cache -E
```

### 9. [Ranger] : Ranger同步时报找不到认证文件

/usr/hdp/current/ranger-usersync/conf/mytruststore.jks文件并不存在。

解决方案：
```
$JAVA_HOME/bin/keytool -import -trustcacerts -alias root -file /usr/hdp/current/ranger-usersync/conf/ad-devsvr.wesuretest.cn-1.cer -keystore /usr/hdp/current/ranger-usersync/conf/mytruststore.jks
```

ad-devsvr.wesuretest.cn-1.cer是从AD服务器上下载的认证文件。

注：输入过程中可能会要求输入密码，此密码需要配置到ranger.usersync.truststore.password中

### 10. [Hive] : hive上执行mapreduce操作报错：
```
Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask (state=08S01,code=2)
```

查看yarn上日志如下：
```
Application application_1533553019204_0001 failed 2 times due to AM Container for appattempt_1533553019204_0001_000002 exited with exitCode: 1
For more detailed output, check application tracking page:http://cvm-os-devsvr-spark1:8088/proxy/application_1533553019204_0001/Then, click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_1533553019204_0001_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1:
at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
at org.apache.hadoop.util.Shell.run(Shell.java:507)
at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:399)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Shell output: main : command provided 1
main : run as user is hive
main : requested yarn user is hive
Writing to tmp file /yarn/nm/nmPrivate/application_1533553019204_0001/container_1533553019204_0001_02_000001/container_1533553019204_0001_02_000001.pid.tmp
Container exited with a non-zero exit code 1
Failing this attempt. Failing the application.
```

程序测试：

```
[root@cvm-os-devsvr-spark1 hadoop-mapreduce]# pwd
/opt/cdh/cloudera/parcels/CDH/lib/hadoop-mapreduce
[root@cvm-os-devsvr-spark1 hadoop-mapreduce]# yarn jar hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar pi 2 4

```

**分析与解决：**
使用yarn logs查看，发现如下错误：
```

...  ...

    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceInit(JobHistoryEventHandler.java:183)
    ... 10 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=hive, access=EXECUTE, inode="/user/history":mapred:supergroup:drwxrwx---
    at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:279)
...  ...

```

临时解决方案：
```
hadoop dfs -chmod -R 777 /user/history
```
